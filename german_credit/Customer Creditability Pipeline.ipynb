{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark://master-172-31-47-102:7077\n"
     ]
    }
   ],
   "source": [
    "# Assign name to the job\n",
    "sc.appName='Cedit_Risk_Clssification'\n",
    "print sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# local file direcory for the input directory\n",
    "#project_root='file:///home/ubuntu/spark/Notebooks/german_credit/'\n",
    "# HDFS path for the input directory\n",
    "hdfs_project_root='german_credit/'\n",
    "# Name of the input file\n",
    "raw_data_file=hdfs_project_root+'german_credit.csv'\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Read file from hdfs\n",
    "raw_data = sc.textFile(raw_data_file)\n",
    "# Read first line as header and replace spaces from them\n",
    "header=raw_data.first().replace(' ','_')\n",
    "\n",
    "# remove header from actual data\n",
    "only_data = raw_data.filter(lambda r : r.replace(' ','_') != header)\n",
    "\n",
    "# By deafault everything is read as string,convert the data to double/float\n",
    "def to_type(data):\n",
    "    return [float(i) for i in data]\n",
    "\n",
    "#only_data=only_data.map(lambda r : r.split(',')).map(to_type).collect()\n",
    "#print only_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import the inbuilt data types from pyspark.sql module\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#This line if you want the deafault header in the schema\n",
    "#fields = [StructField(field_name,DoubleType(), True) for field_name in header.split(',')]\n",
    "#schema = StructType(fields)\n",
    "\n",
    "# Changing the header as the deafault headers are lengthy \n",
    "schema = StructType([StructField('creditability', DoubleType(), True),\n",
    "                     StructField('balance', DoubleType(), True),\n",
    "                     StructField('duration', DoubleType(), True),\n",
    "                     StructField('history', DoubleType(), True),\n",
    "                     StructField('purpose', DoubleType(), True),\n",
    "                     StructField('amount', DoubleType(), True),\n",
    "                     StructField('savings', DoubleType(), True),\n",
    "                     StructField('employment', DoubleType(), True),\n",
    "                     StructField('instPercent', DoubleType(), True),\n",
    "                     StructField('sexMarried', DoubleType(), True),\n",
    "                     StructField('guarantors', DoubleType(), True),\n",
    "                     StructField('residenceDuration', DoubleType(), True),\n",
    "                     StructField('assets', DoubleType(), True),\n",
    "                     StructField('age', DoubleType(), True),\n",
    "                     StructField('concCredit', DoubleType(), True),\n",
    "                     StructField('apartment', DoubleType(), True),\n",
    "                     StructField('credits', DoubleType(), True),\n",
    "                     StructField('occupation', DoubleType(), True),\n",
    "                     StructField('dependents', DoubleType(), True),\n",
    "                     StructField('hasPhone', DoubleType(), True),\n",
    "                     StructField('foreign', DoubleType(), True)\n",
    "                    ])\n",
    "# Map the schema to the data and create data frame\n",
    "Customers = sqlContext.createDataFrame(only_data, schema)\n",
    "\n",
    "#check schema\n",
    "Customers.printSchema()\n",
    "# Check the actual data now\n",
    "Customers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+------------------+------------------+\n",
      "|creditability|        avgbalance|            avgamt|            avgdur|\n",
      "+-------------+------------------+------------------+------------------+\n",
      "|          1.0|2.8657142857142857| 2985.442857142857|19.207142857142856|\n",
      "|          0.0|1.9033333333333333|3938.1266666666666|             24.86|\n",
      "+-------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# register the data frame as table\n",
    "Customers.registerTempTable(\"credit\")\n",
    "# query the credability table to check average balance amount,average loan and average duration for \n",
    "# each class of customer i.e. 1 and 0\n",
    "results =  sqlContext.sql(\"SELECT creditability, avg(balance) as avgbalance, avg(amount) as avgamt, \\\n",
    "                          avg(duration) as avgdur  FROM credit GROUP BY creditability \")\n",
    "# cheack the result of the query\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|          balance|\n",
      "+-------+-----------------+\n",
      "|  count|             1000|\n",
      "|   mean|            2.577|\n",
      "| stddev|1.257637727110893|\n",
      "|    min|              1.0|\n",
      "|    max|              4.0|\n",
      "+-------+-----------------+\n",
      "\n",
      "+-------------+------------------+\n",
      "|creditability|      avg(balance)|\n",
      "+-------------+------------------+\n",
      "|          1.0|2.8657142857142857|\n",
      "|          0.0|1.9033333333333333|\n",
      "+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check summary for the column balance\n",
    "Customers.describe(\"balance\").show()\n",
    "# use domain specific functions to query\n",
    "Customers.groupBy(\"creditability\").avg(\"balance\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(features=DenseVector([1.0, 18.0, 4.0, 2.0, 1049.0, 1.0, 2.0, 4.0, 2.0, 1.0, 4.0, 2.0, 21.0, 3.0, 1.0, 1.0, 3.0, 1.0, 1.0, 1.0]), creditability=1.0), Row(features=DenseVector([1.0, 9.0, 4.0, 0.0, 2799.0, 1.0, 3.0, 2.0, 3.0, 1.0, 2.0, 1.0, 36.0, 3.0, 1.0, 2.0, 3.0, 2.0, 1.0, 1.0]), creditability=1.0), Row(features=DenseVector([2.0, 12.0, 2.0, 9.0, 841.0, 2.0, 4.0, 2.0, 2.0, 1.0, 4.0, 1.0, 23.0, 3.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0]), creditability=1.0), Row(features=DenseVector([1.0, 12.0, 4.0, 0.0, 2122.0, 1.0, 3.0, 3.0, 3.0, 1.0, 2.0, 1.0, 39.0, 3.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0]), creditability=1.0), Row(features=DenseVector([1.0, 12.0, 4.0, 0.0, 2171.0, 1.0, 3.0, 4.0, 3.0, 1.0, 4.0, 2.0, 38.0, 1.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0]), creditability=1.0)]\n"
     ]
    }
   ],
   "source": [
    "# Explicitly form an array with all the feature names \n",
    "featureCols = [\"balance\", \"duration\", \"history\", \"purpose\", \"amount\",\n",
    "      \"savings\", \"employment\", \"instPercent\", \"sexMarried\", \"guarantors\",\n",
    "      \"residenceDuration\", \"assets\", \"age\", \"concCredit\", \"apartment\",\n",
    "      \"credits\", \"occupation\", \"dependents\", \"hasPhone\", \"foreign\"]\n",
    "\n",
    "# import VectorAssembler to convert entire data frame into vector representation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# pass the settings for the vectoriser.\n",
    "# pass the input columns to be vectorised and then pass the name of the output column which would hold the vectorised\n",
    "# output. The output column will have features stored in a DenseVector data structure \n",
    "assembler=VectorAssembler(inputCols=featureCols,\n",
    "                          outputCol=\"features\"\n",
    "                          )\n",
    "# Transform the data as per the vectoriser defined\n",
    "output = assembler.transform(Customers)\n",
    "# Check the data structure\n",
    "print(output.select(\"features\", \"creditability\").take(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the tarnsformer to be used with the labels\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "# Define the indexer for the label with input and output column\n",
    "indexer = StringIndexer(inputCol=\"creditability\", outputCol=\"label\")\n",
    "# tarnsform the label\n",
    "indexed = indexer.fit(output).transform(output)\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the training data into train and validation set\n",
    "splitSeed = 5043\n",
    "trainingData, testData = indexed.randomSplit([0.7, 0.3], splitSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the training data in a vectorised form which most of the ML algorithms Understands.\n",
    "Also we have the validation data in the same form of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|  1.0|[2.0,36.0,2.0,5.0...|\n",
      "|       0.0|  0.0|[1.0,6.0,4.0,0.0,...|\n",
      "|       0.0|  0.0|[1.0,8.0,4.0,0.0,...|\n",
      "|       0.0|  0.0|[1.0,11.0,4.0,0.0...|\n",
      "|       0.0|  0.0|[2.0,36.0,4.0,3.0...|\n",
      "|       0.0|  1.0|[4.0,18.0,4.0,6.0...|\n",
      "|       0.0|  0.0|[1.0,15.0,2.0,3.0...|\n",
      "|       0.0|  0.0|[2.0,18.0,2.0,6.0...|\n",
      "|       0.0|  0.0|[2.0,24.0,4.0,9.0...|\n",
      "|       0.0|  0.0|[3.0,36.0,2.0,3.0...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import RandomForest classifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Define the classifier with all the ncessary settings and hyper-parameters\n",
    "classifier = RandomForestClassifier(numTrees=20, maxDepth=3, labelCol=\"label\", seed=42,impurity=\"gini\", \\\n",
    "                                    featureSubsetStrategy=\"auto\")\n",
    "# Pass the data to the Random Forst classfier function for the transformation which would return the tranformed \n",
    "# values aka model\n",
    "model = classifier.fit(trainingData)\n",
    "\n",
    "# Now pass the new data to the learned model to see the prediction\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Since the predictions is a data frame we can use the the domain specific function to query the predicted label ,\n",
    "# actual label and the actual vectorised data \n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have successfuly build a Random Forest classfication model to categorise a bank customer's credebility. Now let's check its performance with some evaluation  metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's check the misclassification ratio by comparing the actual label to the predicted label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('accuracy before pipeline fitting', 0.750615763546798)\n"
     ]
    }
   ],
   "source": [
    "# Import an evaluator module for binary classification\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "# define the settings for the evaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
    "# pass the test data for the evaluation\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"accuracy before pipeline fitting\" ,accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Also if needed you can check the statistical significance of the model by checking other errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('MSE: ', 0.31772575250836116)\n",
      "('MAE: ', 0.3177257525083612)\n",
      "('RMSE Squared: ', 0.563671670840713)\n",
      "('R Squared: ', -0.4575636288998355)\n",
      "('Explained Variance: ', 0.12343262379615441)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "ab = predictions.select(\"prediction\",\"label\")\n",
    "rm =  RegressionMetrics(ab.map(lambda a:(a[0],a[1])))\n",
    "\n",
    "print(\"MSE: \" , rm.meanSquaredError)\n",
    "print(\"MAE: \" , rm.meanAbsoluteError)\n",
    "print(\"RMSE Squared: \" , rm.rootMeanSquaredError)\n",
    "print(\"R Squared: \" , rm.r2)\n",
    "print(\"Explained Variance: \" , rm.explainedVariance )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Till now we have build and validated the model with a fixed set of hyper-meters.But in a real apllication we should \n",
    "#### try a set of parameters before finalising the model. This procedure is known as Model-Selection.\n",
    "\n",
    "#### We will perform the model selction with parameter grid which has multiple values for the hyper-parametrs and it gets \n",
    "#### passed to the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('accuracy after pipeline fitting', 0.7671130952380953)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "grid = ParamGridBuilder() \\\n",
    "      .addGrid(classifier.maxBins, [35, 40]) \\\n",
    "      .addGrid(classifier.maxDepth, [9, 18]) \\\n",
    "      .addGrid(classifier.numTrees, [50, 80]) \\\n",
    "      .addGrid(classifier.impurity, [\"entropy\", \"gini\"]) \\\n",
    "      .build()\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "cv = CrossValidator(estimator=classifier, estimatorParamMaps=grid, evaluator=evaluator,numFolds=10)\n",
    "pipelineFittedModel = cv.fit(trainingData)\n",
    "\n",
    "predictions2 = pipelineFittedModel.transform(testData)\n",
    "accuracy2 = evaluator.evaluate(predictions2)\n",
    "print(\"accuracy after pipeline fitting\" , accuracy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As we can see there is slight change in accuracy. Right now e have the pipeline built and ready for the classification.\n",
    "### To Enhance the accuracy any further you can tune the features and play around with parameters your self."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('MSE: ', 0.2809364548494983)\n",
      "('MAE: ', 0.2809364548494983)\n",
      "('RMSE Squared: ', 0.5300343902517065)\n",
      "('R Squared: ', -0.2887931034482758)\n",
      "('Explained Variance: ', 0.1605351170568561)\n"
     ]
    }
   ],
   "source": [
    "ab2 = predictions2.select(\"prediction\",\"label\")\n",
    "rm2 =  RegressionMetrics(ab2.map(lambda a:(a[0],a[1])))\n",
    "\n",
    "print(\"MSE: \" , rm2.meanSquaredError)\n",
    "print(\"MAE: \" , rm2.meanAbsoluteError)\n",
    "print(\"RMSE Squared: \" , rm2.rootMeanSquaredError)\n",
    "print(\"R Squared: \" , rm2.r2)\n",
    "print(\"Explained Variance: \" , rm2.explainedVariance )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
